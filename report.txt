Drug-target interaction (DTI) prediction is a critical step in the drug discovery process, enabling researchers to identify potential drug candidates and predict their effectiveness in targeting specific disease-causing proteins. Accurate DTI prediction is essential for guiding rational drug development strategies and accelerating the translation of promising drug candidates into clinical trials.

Traditional approaches to DTI prediction have relied on ligand-based methods, which utilize chemical similarity measures to identify potential interactions, and structure-based methods, which employ protein-ligand docking simulations to assess the physical compatibility of drug molecules and protein targets. However, these methods have limitations in handling the diverse and complex nature of chemical structures and biological interactions.

Recurrent neural network (RNN) autoencoders have emerged as a promising tool for learning efficient representations of molecules, capturing their underlying molecular features and patterns. These representations can then be used to predict DTIs by comparing the similarity between the encoded representations of drug molecules and target proteins.

In this study, we propose a novel approach to DTI prediction using molecule RNN autoencoders. This method employs RNN autoencoders to learn representations from both molecular and target structures, allowing for a more comprehensive assessment of potential interactions. The proposed method is trained on a dataset of molecules, where the RNN autoencoder captures the intrinsic structure of each molecule.

The integration of molecule RNN autoencoders into DTI prediction has the potential to significantly improve the accuracy and predictive power of these methods. By capturing the intricate features of both molecules and targets, this approach can better identify and predict interactions that might be missed by traditional methods. This advancement could revolutionize drug discovery by enabling the identification of novel drug candidates and reducing the time and cost associated with drug development.

\subsubsection{Preprocessing}
Both datasets had some transformations applied. However, SMILES preprocessing was common to both. Fisrt of all, we removed any information present in the SMILES string that was not related to the chemical properties relevant for this work. This includes the removal of any labels associated to atoms (e.g. atom numeration), information about isotopic forms of atoms (e.g. $^{13}C$) (mass number does not affect the chemical properties of the atom itself, and radioactive isotopes are frequently present in datasets due to their usage as markers in experiments), and any associated salts. Arguably, this last one could have some importance in the drug effectiveness, due to the fact that the salt form of a drug can affect its solubility and bioavailability. However, we intend to focus solely on the drug-target interaction, and therefore we ignore bioavailability and remove any salts associated to the drug, keeping only the ligand itself. Please note that stereochemistry is preserved, as this is of critical importance for the molecular geometry and therefore the interactions studied here.
Next, we applied a tokenization process to the SMILES strings. This process consists of splitting the SMILES string into its constituent parts, which are then mapped to a vocabulary of tokens. This vocabulary was derived from the large ZINC dataset and is composed of not only the individual elements (organic and heteroatoms, both normal and aromatic forms) (e.g. C, c, N, O, etc.), but also of some functional groups (e.g. -NH2, -NH3, -OH, etc.), the stereocenters (e.g. [C@], [C@@], etc.), the general SMILES notation punctuation (e.g. -, =, \#, etc.), and a starting and ending/padding token ('G' and 'A', respectively). There are 83 tokens in total. Tokenization starts with splitting the string in the relevant tokens (in a greedy strategy, largest tokens first), then prepending a start token and appending one or more padding tokens until the a maximum number is met. Here, the maximum number of tokens was set to 100, after analysis of the datasets. In the rare ocasion one of the SMILES strings was larger than 100 tokens, its was removed from the dataset (truncation would originate mostly invalid molecules, and certainly with very different chemical properties). The resulting tokenized SMILES strings were then mapped to integers.

After these common steps, no further transformations were applied to the SMILES, except for their encoding as one-hot vector to serve as targets to the autoencoder.
We further processed the A_{2A} dataset. The pCHEMBL value, used as a target for the predictor, was normalized to a range of 0 to 1. This was done by subtracting the minimum value and dividing by the maximum value.

\subsubsection{pCHEMBL}
The pCHEMBL value is a measure of the binding affinity of a drug to a target. It is defined as the negative logarithm of one of the most common binding affinity metrics, such as the dissociation constant (Kd), the inhibition constant (Ki), or the half maximal inhibitory concentration (IC50), among others. Although somewhat arbitrary (due to the differences between the underlying metrics), it has been widely validated by literature, while allowing for bigger datasets, and therefore will be used here too.

\subsection{Predictor}

The predictor used in this work is a simple feed-forward neural network, with 2 hidden layers of 512 and 256 neurons, respectively, and a final output layer with a single neuron. The activation function used in the hidden layers was the rectified linear unit (ReLU), while the output layer used a linear activation function. The loss function used was the mean squared error (MSE), and the optimizer was the Adam optimizer. The model was trained for 100 epochs with a batch size of 32, but early stopping was used, with a patience of 10 epochs.

