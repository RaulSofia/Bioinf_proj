Drug-target interaction (DTI) prediction is a critical step in the drug discovery process, enabling researchers to identify potential drug candidates and predict their effectiveness in targeting specific disease-causing proteins. Accurate DTI prediction is essential for guiding rational drug development strategies and accelerating the translation of promising drug candidates into clinical trials.

Traditional approaches to DTI prediction have relied on ligand-based methods, which utilize chemical similarity measures to identify potential interactions, and structure-based methods, which employ protein-ligand docking simulations to assess the physical compatibility of drug molecules and protein targets. However, these methods have limitations in handling the diverse and complex nature of chemical structures and biological interactions.

Recurrent neural network (RNN) autoencoders have emerged as a promising tool for learning efficient representations of molecules, capturing their underlying molecular features and patterns. These representations can then be used to predict DTIs by comparing the similarity between the encoded representations of drug molecules and target proteins.

In this study, we propose a novel approach to DTI prediction using molecule RNN autoencoders. This method employs RNN autoencoders to learn representations from both molecular and target structures, allowing for a more comprehensive assessment of potential interactions. The proposed method is trained on a dataset of molecules, where the RNN autoencoder captures the intrinsic structure of each molecule.

The integration of molecule RNN autoencoders into DTI prediction has the potential to significantly improve the accuracy and predictive power of these methods. By capturing the intricate features of both molecules and targets, this approach can better identify and predict interactions that might be missed by traditional methods. This advancement could revolutionize drug discovery by enabling the identification of novel drug candidates and reducing the time and cost associated with drug development.

\subsubsection{Preprocessing}
Both datasets had some transformations applied. However, SMILES preprocessing was common to both. Fisrt of all, we removed any information present in the SMILES string that was not related to the chemical properties relevant for this work. This includes the removal of any labels associated to atoms (e.g. atom numeration), information about isotopic forms of atoms (e.g. $^{13}C$) (mass number does not affect the chemical properties of the atom itself, and radioactive isotopes are frequently present in datasets due to their usage as markers in experiments), and any associated salts. Arguably, this last one could have some importance in the drug effectiveness, due to the fact that the salt form of a drug can affect its solubility and bioavailability. However, we intend to focus solely on the drug-target interaction, and therefore we ignore bioavailability and remove any salts associated to the drug, keeping only the ligand itself. Please note that stereochemistry is preserved, as this is of critical importance for the molecular geometry and therefore the interactions studied here.
Next, we applied a tokenization process to the SMILES strings. This process consists of splitting the SMILES string into its constituent parts, which are then mapped to a vocabulary of tokens. This vocabulary was derived from the large ZINC dataset and is composed of not only the individual elements (organic and heteroatoms, both normal and aromatic forms) (e.g. C, c, N, O, etc.), but also of some functional groups (e.g. -NH2, -NH3, -OH, etc.), the stereocenters (e.g. [C@], [C@@], etc.), the general SMILES notation punctuation (e.g. -, =, \#, etc.), and a starting and ending/padding token ('G' and 'A', respectively). There are 83 tokens in total. Tokenization starts with splitting the string in the relevant tokens (in a greedy strategy, largest tokens first), then prepending a start token and appending one or more padding tokens until the a maximum number is met. Here, the maximum number of tokens was set to 100, after analysis of the datasets. In the rare ocasion one of the SMILES strings was larger than 100 tokens, its was removed from the dataset (truncation would originate mostly invalid molecules, and certainly with very different chemical properties). The resulting tokenized SMILES strings were then mapped to integers.

After these common steps, no further transformations were applied to the SMILES, except for their encoding as one-hot vector to serve as targets to the autoencoder.
We further processed the A_{2A} dataset. The pCHEMBL value, used as a target for the predictor, was normalized to a range of 0 to 1. This was done by subtracting the minimum value and dividing by the maximum value.

\subsubsection{pCHEMBL}
The pCHEMBL value is a measure of the binding affinity of a drug to a target. It is defined as the negative logarithm of one of the most common binding affinity metrics, such as the dissociation constant (Kd), the inhibition constant (Ki), or the half maximal inhibitory concentration (IC50), among others. Although somewhat arbitrary (due to the differences between the underlying metrics), it has been widely validated by literature, while allowing for bigger datasets, and therefore will be used here too.

\subsection{Predictor}

The predictor used in this work is a simple feed-forward neural network, with 2 hidden layers of 512 and 256 neurons, respectively, and a final output layer with a single neuron. The activation function used in the hidden layers was the rectified linear unit (ReLU), while the output layer used a linear activation function. The loss function used was the mean squared error (MSE), and the optimizer was the Adam optimizer. The model was trained for 100 epochs with a batch size of 32, but early stopping was used, with a patience of 10 epochs.

\subsection{Fine-tuning: Freezing weights vs Retraining vs Reinitializing model}

In this experiment, three strategies were adopted in order to fine-tune the model. The first one was to freeze the weights of the encoder and only train the predictor (the main goal of this study). The second one was to retrain the whole model, encoder and predictor, starting with the pretrained weights (the encoder weights were not frozen). The third one was to reinitialize the whole model, encoder and predictor, and train it from scratch. The last two strategies were used as a baseline, to compare against the performance of the first one. The rationale behind this is that the encoder weights, being pretrained, should already be close to the optimal weights, and therefore should not need much change. The predictor, on the other hand, should be able to learn the task starting from the encoded representation of the molecules, and therefore, ideally, should not need to change the pretrained weights. The third strategy was used to assess the importance of the pretraining over a larger dataset (ZINC) in the performance of the model.

We tested these strategies on best performing encoder model, using 128-dimentional latent space. We also compared them against the LSTM architecture proposed by ***.
The results obtained were the following:

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Strategy} & \textbf{MSE} & \textbf{CCC} & \textbf{$r^2$} \\ \hline
Freezing & 0.0163 & 0.5989 & 0.4502 \\ \hline
Retraining & \textbf{0.0110} & \textbf{0.7811} & \textbf{0.6255} \\ \hline
Reinitializing & 0.0115 & 0.7643 & 0.6204 \\ \hline
LSTM predictor & 0.0128 & 0.7373 & 0.5366 \\ \hline
\end{tabular}
\caption{Mean Squared Error achieved for each of the different strategies}
\label{tab:ft}
\end{table}

As we can see, the best performing strategy was to retrain the whole model, encoder and predictor, starting with the pretrained weights. This is in line with our expectations, as the encoder weights should already be close to the optimal weights. We didn't expect such a poor performance from the freezing strategy (the classical transfer learning approach), but this might arise from the fact that the encoder was trained on a very different dataset, and therefore the encoded representation might not be that optimal for the task at hand. It might be that the encoded representations eliminate some of the information relevant for the task, as it was not explicitly trained for it. The reinitializing strategy, on the other hand, performed very well, almost as well as the retraining strategy. This was more or less expected, as they share the exact same architecture. The small difference may come from the already lightly optimized weights of the encoder in the pretraining case (the reinitializing strategy starts with random weights). The pretrained network already produced a reasonable encoded representation of the molecules, but, differently from the freezing case, the encoder and the predictor were allowed to co-adapt for the new task, achieving the best performance. As a comparison to other existing models, we also trained a LSTM predictor from *** on the same dataset. This network, although with similar layers to the networks used here, is considerably less complex, and therefore the poorer performance is not surprising. Nevertheless, it is still good to note the improvement.


\subsection{Varying the encoding dimension of the encoder}
To test the effect of the encoding dimension of in the performance of the predictor three encoding models were used only varying the size of the encoding dimension. We chose the following sizes: 128, 64 and 32. The three sizes were tested with the same predictor, and the results obtained were the following:

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Encoding dimension} & \textbf{MSE} & \textbf{CCC} & \textbf{$r^2$} \\ \hline
128 & 0.0110\\ \hline
64 & 0.0144\\ \hline
32 & 0.0153\\ \hline
\end{tabular}
\caption{Mean Squared Error achieved for each of the different encoding dimensions}
\label{tab:enc_dim}
\end{table}





As expected, higher encoding dimensions lead to better results in the prediction task, this effect might be an expression of several effects, mainly the reduction of the bottleneck, allowing to pass-through information in an unchanged manner and the simple increase in model size resulting from the higher amount of parameters.





\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{avgerror.png}
    \caption{Error for the predictor with latent dimension of 128 and pre-trained non-freezed encoder.}
    \label{fig:avgerror}
\end{figure}


